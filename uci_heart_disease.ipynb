{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c23b256aed44dd4d",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "[1. Install scikit-learn & required library](#1-install-scikit-learn--required-library)<br>\n",
    "[2. Preparing the datasets](#2-preparing-the-datasets)<br>\n",
    "[3. Prepare, Building and Evaluating with the decision tree](#3-prepare-building-and-evaluating-with-the-decision-tree)<br>\n",
    "⠀⠀[3.1 Prepare the splits for the building of the decision tree](#31-prepare-the-splits-for-the-building-of-the-decision-tree)<br>\n",
    "⠀⠀[3.2 Training and evaluate](#32-training-and-evaluate)<br>\n",
    "⠀⠀[3.3 Run all splits](#33-run-all-splits)<br>\n",
    "⠀⠀[3.4 Depth analysis](#34-depth-analysis)<br>\n",
    "\n",
    "[To some Internal Section](#section_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d560f090b8e6b00c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Install scikit-learn & required library\n",
    "<a id='another_cell'></a>\n",
    "This project require numpy, pandas, matplotlib,... and scikit-learn be installed. Run the following code to install the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95fd6b4ea0500a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -U scikit-learn\n",
    "!python -m pip install matplotlib\n",
    "!python -m pip install graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed921fa4808fe546",
   "metadata": {},
   "source": [
    "# 2. Preparing the datasets\n",
    "The following code will download the dataset heart-disease from URL.\n",
    "\n",
    "• Binary class dataset: The [UCI Heart Disease dataset](https://archive.ics.uci.edu/dataset/45/heart+disease) is used for classifying whether a\n",
    "patient has a heart disease or not based on age, blood pressure, cholesterol level, and other\n",
    "medical indicators. This dataset includes 303 samples, with labels indicating presence (1) or\n",
    "absence (0) of heart disease. Experiments with the Cleveland database have concentrated on\n",
    "simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).\n",
    "\n",
    "The dataset will be fetched from URL then split into `feature` and `label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4379cebc6a1798",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease_db_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "heart_disease_columns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\"]\n",
    "dataset_name = \"Heart Disease\"\n",
    "\n",
    "def split_dataset(_dataset: pd.DataFrame, targets: list):\n",
    "\t\"\"\"\n",
    "\tSplit input dataset into feature and target by input targets\n",
    "\t:return: dict[\"feature\"]\n",
    "\t\"\"\"\n",
    "\texisting_columns = [col for col in targets if col in _dataset.columns]\n",
    "\tmissing_columns = [col for col in targets if col not in _dataset.columns]\n",
    "\tif missing_columns:\n",
    "\t\tprint(\"These columns are not found in the dataset:\", missing_columns)\n",
    "\treturn {\n",
    "\t\t\"feature\": _dataset.drop(existing_columns, axis=1),\n",
    "\t\t\"target\": _dataset[existing_columns],\n",
    "\t}\n",
    "\n",
    "# fetch dataset from url\n",
    "raw_heart_db = pd.read_csv(heart_disease_db_url, names=heart_disease_columns)\n",
    "raw_heart_db = raw_heart_db.replace('?', np.nan)\n",
    "raw_heart_db = raw_heart_db.dropna()\n",
    "raw_heart_db = raw_heart_db.astype(float)\n",
    "raw_heart_db['num'] = raw_heart_db['num'].apply(lambda x: 1 if x > 0 else 0) # labelling the target 'num'\n",
    "\n",
    "dataset = split_dataset(raw_heart_db, targets=['num'])  # adding columns name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd6805985979e1",
   "metadata": {},
   "source": [
    "# 3. Prepare, Building and Evaluating with the decision tree\n",
    "> Required to run the code at [Section #2](#2-preparing-the-datasets) before continue!\n",
    "\n",
    "### 3.1 Prepare the splits for the building of the decision tree\n",
    "This following code splits the dataset into multiple splits with defined ratio.\n",
    "The splits are structured as following `[ratio: float => (feature_train, feature_test, label_train, label_test)]`\n",
    "<a id='section_id'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708c9a6871d0c062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "split_ratios = [0.6, 0.4, 0.2, 0.1] # train/test 60/40 40/60 80/20 90/10\n",
    "random_seed = None                  # 42 for testing and cultural reference :)\n",
    "class_name = [\"No Decease\", \"Decease\"]\n",
    "\n",
    "def tee_print(message, file):\n",
    "\tprint(message)            # print to console\n",
    "\tprint(message, file=file) # print to log file\n",
    "\n",
    "def snake_case(s: str) -> str:\n",
    "\treturn ''.join('_' if c.isspace() else c.lower() for c in s)\n",
    "\n",
    "snake_cased_dataset_name = snake_case(dataset_name)\n",
    "os.makedirs(f\"output/depth/{snake_cased_dataset_name}\", exist_ok=True)\n",
    "os.makedirs(f\"output/splits/{snake_cased_dataset_name}\", exist_ok=True)\n",
    "\n",
    "def prepare_dataset(features, labels, test_size, seed=None):\n",
    "\t\"\"\"\n",
    "\t:param features: input features\n",
    "\t:param labels: input labels\n",
    "\t:param test_size: Test size ratio (test/(train+test))\n",
    "\t:param seed: seed used for random, default=None for random seed\n",
    "\t:return: feature_train, feature_test, label_train, label_test\n",
    "\t\"\"\"\n",
    "\treturn train_test_split(features, labels, test_size=test_size, stratify=labels, random_state=seed, shuffle=True)\n",
    "\n",
    "def prepare_all_splits(features, labels, seed=None):\n",
    "    splits = {}\n",
    "    for split_ratio in split_ratios:\n",
    "        splits[split_ratio] = prepare_dataset(features, labels, test_size=split_ratio, seed=seed)\n",
    "    return splits\n",
    "\n",
    "dataset_splits = prepare_all_splits(dataset['feature'], dataset['target'], seed=random_seed)\n",
    "\n",
    "print(dataset_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5999286da085aa4",
   "metadata": {},
   "source": [
    "#### 3.1.a Visualize the class distributions in all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff8b3eb8bbf92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_class_distribution():\n",
    "\t# Prepare figure with subplots and white background\n",
    "\tfig, axes = plt.subplots(2, 2, figsize=(12, 8), facecolor='white')\n",
    "\taxes = axes.flatten()\n",
    "\n",
    "\tfor i, test_size in enumerate(split_ratios):\n",
    "\t\t_feature_train, _feature_test, _label_train, _label_test = dataset_splits[test_size]\n",
    "\n",
    "\t\toriginal_counts = dataset['target']['num'].value_counts() #.reindex(class_name)\n",
    "\t\ttrain_counts = pd.Series(_label_train['num']).value_counts() #.reindex(class_name)\n",
    "\t\ttest_counts = pd.Series(_label_test['num']).value_counts() #.reindex(class_name)\n",
    "\t\tdf = pd.DataFrame({\n",
    "\t\t    'Original': original_counts,\n",
    "\t        'Train': train_counts,\n",
    "\t        'Test': test_counts\n",
    "\t\t})\n",
    "\n",
    "\t    # plot stacked bar chart\n",
    "\t\tax = axes[i]\n",
    "\t\tbars = df.T.plot(kind='bar', stacked=True, ax=ax, color=['#1f77b4', '#ff7f0e'], width=0.8)\n",
    "\n",
    "\t\t# add counters (text labels) to each segment\n",
    "\t\tfor bar in bars.patches:\n",
    "\t\t    height = bar.get_height()\n",
    "\t\t    width = bar.get_width()\n",
    "\t\t    x = bar.get_x()\n",
    "\t\t    y = bar.get_y()\n",
    "\t\t    if height > 0:  # Only add label if segment exists\n",
    "\t\t        label = f'{int(height)}'\n",
    "\t\t        ax.text(\n",
    "\t\t            x + width / 2, y + height / 2, label,\n",
    "\t\t            ha='center', va='center', color='white', fontsize=10, fontweight='bold'\n",
    "\t\t        )\n",
    "\t\t# Customize plot\n",
    "\t\tax.set_facecolor('white')\n",
    "\t\t# Set axes background to white\n",
    "\t\tax.set_title(f'Train/Test Split: {round((1-test_size)*100)}/{round(test_size*100)}')\n",
    "\t\tax.set_ylabel('Count')\n",
    "\t\tax.set_xlabel('Dataset')\n",
    "\t\tax.legend(title='Class', labels=class_name)\n",
    "\t\tax.tick_params(axis='x', rotation=0)\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f\"output/splits/{snake_cased_dataset_name}/split_distribution_{snake_cased_dataset_name}.png\")\n",
    "\tplt.show()\n",
    "visual_class_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e78548f191ed8",
   "metadata": {},
   "source": [
    "### 3.2 Training and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8060411673ddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def build_id3_tree(feature_train, label_train, max_depth=None) -> DecisionTreeClassifier:\n",
    "\tdtc = DecisionTreeClassifier(criterion='entropy', random_state=random_seed, max_depth=max_depth)\n",
    "\tdtc.fit(feature_train, label_train) # train\n",
    "\treturn dtc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e4ae13379bb93",
   "metadata": {},
   "source": [
    "### 3.3 Run all splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266aa6c0152f28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree, export_graphviz\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import graphviz\n",
    "\n",
    "def run_all_splits():\n",
    "\twith open(f\"output/splits/{snake_cased_dataset_name}/log.txt\", \"w\") as log_file:\n",
    "\t\tfor test_size in split_ratios:\n",
    "\t\t\t_feature_train, _feature_test, _label_train, _label_test = dataset_splits[test_size]\n",
    "\t\t\tdtc = build_id3_tree(_feature_train, _label_train)\n",
    "\t\t\t# export decision tree to DOT format\n",
    "\t\t\t_dot_data = export_graphviz(dtc, feature_names=_feature_train.columns, class_names=class_name, filled=True, rounded=True, special_characters=True)\n",
    "\t\t\t_graph = graphviz.Source(_dot_data, format=\"svg\")\n",
    "\t\t\ttee_print(f\"=== {dataset_name} Train/Test {100 - round(test_size * 100)}/{round(test_size * 100)} ===\", file=log_file)\n",
    "\t\t\tdisplay(_graph)\n",
    "\n",
    "\t\t\t_output_file = f\"output/splits/{snake_cased_dataset_name}/id3_{snake_cased_dataset_name}_{100 - round(test_size * 100)}_{round(test_size * 100)}\"\n",
    "\t\t\t_graph.render(_output_file, cleanup=True, directory=\".\")\n",
    "\t\t\ttee_print(f\"Saved decision tree visualization: {_output_file}.svg\", file=log_file)\n",
    "\t\t\t_label_predict = dtc.predict(_feature_test) # predict the label of feature_test based on the tree\n",
    "\t\t\ttee_print(classification_report(_label_test, _label_predict, target_names=[\"No Decease\", \"Decease\"]), file=log_file)\n",
    "\n",
    "\t\t\tcm = confusion_matrix(_label_test, _label_predict)\n",
    "\t\t\tdisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_name)\n",
    "\t\t\tfig, ax = plt.subplots(figsize=(8, 6))\n",
    "\t\t\tdisp.plot(ax=ax, cmap='Blues')\n",
    "\t\t\tplt.title(f\"Confusion Matrix (Depth=None, {100 - round(test_size * 100)}/{round(test_size * 100)})\")\n",
    "\t\t\tplt.yticks(rotation=90)\n",
    "\t\t\tplt.grid(False)\n",
    "\t\t\tplt.savefig(f\"output/splits/{snake_cased_dataset_name}/confusion_matrix_{snake_cased_dataset_name}_{100 - round(test_size * 100)}_{round(test_size * 100)}.png\", dpi=150)\n",
    "\t\t\tplt.show()\n",
    "run_all_splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaccac54a1b2dfe",
   "metadata": {},
   "source": [
    "### 3.4 Depth analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d63121840fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "from IPython.display import display\n",
    "\n",
    "def depth_analysis():\n",
    "\tdepths = [None, 2, 3, 4, 5, 6, 7]\n",
    "\taccuracies = []\n",
    "\n",
    "\tfeature_train, feature_test, label_train, label_test = dataset_splits[0.2]\n",
    "\n",
    "\twith open(\"output/depth/heart_disease/log.txt\", \"w\") as log_file:\n",
    "\t\tfor d in depths:\n",
    "\t\t\t# train decision tree with specified max_depth\n",
    "\t\t\tdtc = build_id3_tree(feature_train, label_train, max_depth=d)\n",
    "\t\t\tresult_depth = dtc.get_depth()\n",
    "\t\t\tlabel_predict = dtc.predict(feature_test)\n",
    "\t\t\tacc = accuracy_score(label_test, label_predict)\n",
    "\t\t\taccuracies.append(acc)\n",
    "\n",
    "\t\t\ttee_print(f\"Max Depth: {d} - Actual Depth: {result_depth} - Accuracy: {acc:.4f}\", file=log_file)\n",
    "\t\t\tsys.stdout.flush()\n",
    "\n",
    "\t\t\t# export decision tree to DOT format\n",
    "\t\t\t_dot_data = export_graphviz(dtc, feature_names=feature_train.columns, class_names=class_name, filled=True, rounded=True, special_characters=True)\n",
    "\t\t\t_graph = graphviz.Source(_dot_data, format=\"svg\")\n",
    "\n",
    "\t\t\t_output_file = f\"output/depth/{snake_cased_dataset_name}/id3_{snake_cased_dataset_name}_80_20_depth_{d if d is not None else \"none\"}\"\n",
    "\t\t\t_graph.render(_output_file, cleanup=True, directory=\".\")\n",
    "\t\t\tdisplay(_graph)\n",
    "\t\t\ttee_print(f\"Saved decision tree visualization: {_output_file}.svg\", file=log_file)\n",
    "\t\t\ttee_print(f\"-====================================================-\\n\", file=log_file)\n",
    "\n",
    "\t\t# Plot accuracies\n",
    "\t\tplt.figure(figsize=(10, 6), dpi=150)\n",
    "\t\tplt.plot([str(d) for d in depths], accuracies, marker='o')\n",
    "\t\tplt.title(f\"Accuracy vs Max Depth ({dataset_name})\")\n",
    "\t\tplt.xlabel(\"Max Depth\")\n",
    "\t\tplt.ylabel(\"Accuracy\")\n",
    "\n",
    "\t\tplt.ylim(top=max(accuracies) + 0.02)  # add a bit of headroom\n",
    "\t\t# add value labels above each point\n",
    "\t\tfor x, y in zip([str(d) for d in depths], accuracies):\n",
    "\t\t\tplt.text(x, y + 0.005, f\"{y:.3f}\", ha='center', va='bottom', fontsize=9)\n",
    "\t\tplt.grid()\n",
    "\t\tplt.savefig(f\"output/depth/{snake_cased_dataset_name}/accuracy_{snake_cased_dataset_name}.png\", dpi=150)\n",
    "\t\tplt.show()\n",
    "\t\taccuracy_table = pd.DataFrame({'max_depth': [str(d) for d in depths], 'Accuracy': accuracies})\n",
    "\t\ttee_print(accuracy_table, file=log_file)\n",
    "\n",
    "depth_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
